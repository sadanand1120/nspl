{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "nspl_root_dir = os.environ.get(\"NSPL_REPO\")\n",
    "import numpy as np\n",
    "from terrainseg.inference import TerrainSegFormer\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from simple_colors import green\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "import torchvision\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import SamPredictor\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import SamPredictor\n",
    "from third_party.lightHQSAM.setup_light_hqsam import setup_model\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from terrainseg.inference import TerrainSegFormer\n",
    "import numpy as np\n",
    "import cv2\n",
    "import cv2\n",
    "from pykdtree.kdtree import KDTree\n",
    "from PIL import Image\n",
    "from terrainseg.inference import TerrainSegFormer\n",
    "from third_party.jackal_calib import JackalLidarCamCalibration\n",
    "import open3d as o3d\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from third_party.Depth_Anything.metric_depth.zoedepth.models.builder import build_model\n",
    "from third_party.Depth_Anything.metric_depth.zoedepth.utils.config import get_config\n",
    "torch.set_default_device(\"cuda\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# grid = make_grid(images, nrow=4, padding=20, pad_value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safety.realtime.fast_utils import FastModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "images_dirpath = \"/home/dynamo/AMRL_Research/repos/lifelong_concept_learner/scripts/results/saved/compares/a_new_images\"\n",
    "all_images = os.listdir(images_dirpath)\n",
    "all_images_paths = [os.path.join(images_dirpath, img) for img in all_images]\n",
    "all_images_paths.sort()\n",
    "pil_preds_np = []\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "with torch.inference_mode():\n",
    "    fm = FastModels()\n",
    "    for _, img_path in enumerate(tqdm(all_images_paths)):\n",
    "        pil_img = Image.open(img_path)\n",
    "        cv2_img_np = cv2.cvtColor(np.asarray(pil_img), cv2.COLOR_RGB2BGR)\n",
    "        pred, *_ = fm.predict_new(pil_img)\n",
    "        cv2_overlay_np = TerrainSegFormer.get_seg_overlay(cv2_img_np, pred)\n",
    "        pil_overlay = Image.fromarray(cv2.cvtColor(cv2_overlay_np, cv2.COLOR_BGR2RGB))\n",
    "        pil_preds_np.append(transform(pil_overlay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(pil_preds_np, nrow=6, padding=40, pad_value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_display_images_grid(dir_path):\n",
    "    # Define the transform to convert images to tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Load images from the directory\n",
    "    images = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img_path = os.path.join(dir_path, filename)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            images.append(transform(img))\n",
    "    \n",
    "    # Make a grid of images with white padding\n",
    "    grid = make_grid(images, nrow=6, padding=40, pad_value=1.0)\n",
    "    \n",
    "    # Display the grid\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0))  # Convert from CHW to HWC\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_display_images_grid(images_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
